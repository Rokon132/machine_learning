# -*- coding: utf-8 -*-
"""08_ML_Lab_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14GkGy-RccB-gQ8ycmbA3ednvc9l7YBST
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


def load_data(filepath=None, synthetic=False):
    if synthetic:
        np.random.seed(42)
        x = np.arange(1, 101)
        noise = np.random.normal(0, 50, size=100)
        y = 3 + 5 * x + noise
        data = pd.DataFrame({'x': x, 'y': y})
        data.to_csv('synthetic_data.csv', index=False)
    else:

        data = pd.read_csv(filepath, header=None, names=["x", "y"])
    return data


def scale_data(df):
    """
    Scales the 'x' and 'y' columns using standardization (Z-score).
    Returns the scaled dataframe and the scaler objects for inverse transforming later.
    """
    x_scaler = StandardScaler()
    y_scaler = StandardScaler()

    df_scaled = df.copy()
    df_scaled['x'] = x_scaler.fit_transform(df[['x']])
    df_scaled['y'] = y_scaler.fit_transform(df[['y']])

    return df_scaled, x_scaler, y_scaler


def process_data(data):
    data.insert(0, 'x0', 1)
    X = data[['x0', 'x']].values
    y = data[['y']].values
    return train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)

def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    error = predictions - y
    cost = (1 / (2 * m)) * np.sum(error ** 2)
    return cost

def gradient_descent(X, y, X_val, y_val, alpha=0.000001, iterations=1000):
    m = len(y)
    theta = np.zeros((X.shape[1], 1))
    train_errors = []
    val_errors = []

    for i in range(iterations):
        predictions = X.dot(theta)
        error = predictions - y
        gradient = (1/m) * X.T.dot(error)
        theta -= alpha * gradient

        train_errors.append(compute_cost(X, y, theta) * 2)  # Multiply by 2 to match your MSE reporting
        val_errors.append(compute_cost(X_val, y_val, theta) * 2)

    return theta, train_errors, val_errors

def train(X_train, y_train, X_val, y_val, alpha=0.000001, iterations=1000):
    theta, train_errors, val_errors = gradient_descent(X_train, y_train, X_val, y_val, alpha, iterations)
    return theta, train_errors, val_errors

def evaluate(train_errors, val_errors, theta):
    final_train_error = train_errors[-1]
    final_val_error = val_errors[-1]
    best_val_index = np.argmin(val_errors)

    print(f"Final Training Error: {final_train_error}")
    print(f"Final Validation Error: {final_val_error}")
    print(f"Best Validation Error: {val_errors[best_val_index]}")
    print(f"Corresponding Training Error: {train_errors[best_val_index]}")
    print(f"Learned Parameters (theta): \n{theta}")

    # Error plot
    plt.plot(train_errors, label='Training Error')
    plt.plot(val_errors, label='Validation Error')
    plt.xlabel('Iterations')
    plt.ylabel('MSE')
    plt.title('Training and Validation Error Curves')
    plt.legend()
    plt.show()

def plot_regression(data, theta):
    x_plot = np.linspace(data['x'].min(), data['x'].max(), 100)
    x_plot_with_bias = np.c_[np.ones_like(x_plot), x_plot]
    y_plot = x_plot_with_bias.dot(theta)

    plt.scatter(data['x'], data['y'], color='green', label='Data')
    plt.plot(x_plot, y_plot, color='red', label=f'Regression Line\ny = {theta[0][0]:.2f} + {theta[1][0]:.2f}x')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('Data with Regression Line')
    plt.legend()
    plt.show()

def main(synthetic=True, filepath=None , scaling = False ):
    if scaling:
        data = load_data(filepath, synthetic=False)
        data, x_scaler, y_scaler = scale_data(data)
    else:
        data = load_data(filepath, synthetic)

    X_train, X_val, y_train, y_val = process_data(data)
    # Plot training and validation sets
    plt.scatter(X_train[:, 1], y_train, label='Training', color='green')
    plt.title('Training Data')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.show()

    plt.scatter(X_val[:, 1], y_val, label='Validation', color='red')
    plt.title('Validation Data')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.show()

    theta, train_errors, val_errors = train(X_train, y_train, X_val, y_val)
    evaluate(train_errors, val_errors, theta)
    plot_regression(data, theta)

# Run for synthetic data
main(synthetic=True)

# For real data (uncomment this if you have a data_01.csv file) .
#without scaling real data .
print("\n")
print("\n")
print("without scaling we get the below regression line for real data  ")
main(synthetic=False, filepath='data_01.csv')




# with scaling real data
print("\n")
print("\n")
print("After scaling we get the below regression line ")
main(synthetic=False, filepath='data_01.csv',scaling=True)